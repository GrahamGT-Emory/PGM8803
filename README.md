# DynamicGGM
Learning Gaussian graphical models with temporal dynamics

A popular graphical model representation used to understand real-world, high-dimensional data is the gaussian graphical model (GGM). The GGM provides insights into the conditional independences and relationships between observed variables through the inverse covariance matrix, also called the precision or concentration matrix. One method used to learn GGMs from observed data is the graphical lasso algorithm \cite{friedman2008sparse}. The graphical lasso is a convex optimization problem which optimizes fidelity of the sample precision matrix to the observed data while encouraging sparsity through ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%24%5Cell_1%24) regularization by solving
![equation](http://latex.codecogs.com/gif.latex?%5Cwidehat%7B%5Cbm%7B%5COmega%7D%7D%20%3D%20%7B%5Carg%5Cmin%7D%7E%20-%5Clog%5Cdet%5Cbm%7B%5COmega%7D%20&plus;%20%5Cmathrm%7BTr%7D%5Cleft%5B%5Cbm%7BS%7D%20%5Cbm%7B%5COmega%7D%20%5Cright%5D%20&plus;%20%5Clambda%20%5ClVert%20%5Cbm%7B%5COmega%7D%20%5CrVert_1), where is the recovered precision matrix and ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbm%7BS%7D) is the sample covariance matrix.

One drawback of the graphical lasso method is that, despite using a sparsity-encouraging objective function, the learned GGM is often extremely dense and uninterpretable. In \cite{chandrasekaran2010latent}, Chandrasekaran et al. overcome this issue by hypothesizing that the connections in the observed variables can be explained by a few unobserved, or latent, variables. They decompose the concentration matrix into a sparse observable term ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbm%7BS%7D) and a low-rank hidden term ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbm%7BS%7D) and the solve a similar convex optimization problem, encouraging the sparse and low-rank structure in ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbm%7BS%7D) and ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%24%5Cbm%7BL%7D%24) with ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%24%5Cell_1%24) and nuclear-norm penalties, respectively.

Chandrasekaran et al. assumes the observed data samples are independent and identically distributed. Foti et al. extend this approach to stationary time series data \cite{foti2016sparse}. The observed data is transformed into the frequency domain, where a spectral density matrix is learned using the Whittle likelihood approximation and group ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%24%5Cell_1%24) lasso penalty across frequencies to find structure.    

Although \cite{foti2016sparse} effectively treats time series inputs, it assumes that the underlying GGM is not changing over time (stationarity assumption). After implementing existing GGM methods, we extend these algorithms to the case where we are interested in tracking a GGM using dynamic time series data. To incorporate dynamic information into the graphical lasso, we combine GGM with reweighted ![equation](http://latex.codecogs.com/gif.latex?%5Cinline%20%24%5Cell_1%24) tracking methods.
